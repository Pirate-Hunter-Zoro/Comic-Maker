#!/bin/bash
#SBATCH --partition=c3_accel
#SBATCH --job-name=lora_training
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=48000
#SBATCH --gpus=1
#SBATCH --time=24:00:00
#SBATCH --output=logs/training-%J.stdout
#SBATCH --error=logs/training-%J.stderr

# --- COMMAND VARIABLES (EDIT THESE) ---
# The absolute path to your project's root directory.
PROJECT_ROOT="/home/librad.laureateinstitute.org/mferguson/Visual-Novel-Writer"

# The absolute path to your shared models and output parent directory.
GREAT_ARMORY_PATH="/media/studies/ehr_study/data-EHR-prepped/Mikey-Lora-Trainer"

# The specific project you want to train a LoRA for.
TARGET_PROJECT_SUBPATH="projects/rwby_vacuo_arc"
# --- END OF VARIABLES ---

# --- Dynamic Path and Name Generation ---
# Do not edit below this line.
PROJECT_NAME=$(basename "${TARGET_PROJECT_SUBPATH}")
OUTPUT_NAME="${PROJECT_NAME}_LoRA"
DATASET_CONFIG_FILE="${PROJECT_ROOT}/${TARGET_PROJECT_SUBPATH}/dataset_config.toml"
OUTPUT_DIR="${GREAT_ARMORY_PATH}/${PROJECT_NAME}_LoRA_Output"

# --- The Ritual Begins ---
echo "The Forge awakens on $(hostname)."
echo "Project Target: ${PROJECT_NAME}"

mkdir -p "${PROJECT_ROOT}/logs"

# Load system powers
module load Python/3.11.5-GCCcore-13.2.0
module load CUDA/12.3.0

# Initialize Conda for the non-interactive job shell.
eval "$(conda shell.bash hook)"

# --- THE CORRECTION ---
# Raise the ward against user-site contamination.
export PYTHONNOUSERSITE=1

# Enter your sanctuary
conda activate lora_env

# Go to the beast's lair
cd "${PROJECT_ROOT}/kohya-trainer"

echo "Unleashing the forge command..."

# The command is now fully dynamic.
accelerate launch train_network.py \
--pretrained_model_name_or_path="${GREAT_ARMORY_PATH}/AnyLoRA" \
--dataset_config="${DATASET_CONFIG_FILE}" \
--output_dir="${OUTPUT_DIR}" \
--logging_dir="${OUTPUT_DIR}/_logs" \
--output_name="${OUTPUT_NAME}" \
--log_prefix="${OUTPUT_NAME}" \
--save_precision="fp16" \
--mixed_precision="fp16" \
--network_module=networks.lora \
--network_dim=512 \
--network_alpha=256 \
--resolution=1024 \
--train_batch_size=1 \
--max_train_epochs=15 \
--learning_rate=2e-4 \
--unet_lr=2e-4 \
--text_encoder_lr=6e-5 \
--lr_scheduler="cosine_with_restarts" \
--lr_scheduler_num_cycles=4 \
--lr_warmup_steps=0 \
--optimizer_type="AdamW8bit" \
--save_every_n_steps=10000 \
--clip_skip=2 \
--seed=42 \
--sdpa \
--cache_latents \
--no_half_vae \
--caption_extension=".txt"

echo "The forging is complete."