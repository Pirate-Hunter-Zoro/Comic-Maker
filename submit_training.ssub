#!/bin/bash
#SBATCH --partition=c3_accel
#SBATCH --job-name=lora_training
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=48000
#SBATCH --gpus=1
#SBATCH --time=24:00:00
#SBATCH --output=logs/training-%J.stdout
#SBATCH --error=logs/training-%J.stderr

# --- COMMAND VARIABLES (EDIT THESE) ---
PROJECT_PATH="/home/librad.laureateinstitute.org/mferguson/Visual-Novel-Writer"
GREAT_ARMORY_PATH="/media/studies/ehr_study/data-EHR-prepped/Mikey-Lora-Trainer"
TARGET_PROJECT_SUBPATH="projects/rwby_vacuo_arc"
# --- END OF VARIABLES ---

# --- Dynamic Path and Name Generation ---
PROJECT_NAME=$(basename "${TARGET_PROJECT_SUBPATH}")
OUTPUT_NAME="${PROJECT_NAME}_LoRA"
DATASET_CONFIG_FILE="${PROJECT_PATH}/${TARGET_PROJECT_SUBPATH}/dataset_config.toml"
OUTPUT_DIR="${GREAT_ARMORY_PATH}/${PROJECT_NAME}_LoRA_Output"
VENV_PATH="${PROJECT_PATH}/conda_envs/lora_env"

# --- The Ritual Begins ---
echo "The Forge awakens on $(hostname)."
echo "Project Target: ${PROJECT_NAME}"
mkdir -p "${PROJECT_PATH}/logs"

# Load system powers
module load Python/3.11.5-GCCcore-13.2.0
module load CUDA/12.3.0

# This is the direct incantation to awaken Conda in a new shell.
eval "$(conda shell.bash hook)"

export PYTHONNOUSERSITE=1

# Enter sanctuary by its absolute path
conda activate "${VENV_PATH}"

# Go to the beast's lair
cd "${PROJECT_PATH}/kohya-trainer"

echo "Unleashing the forge command..."

# The command bound to its true location and spirit.
"${VENV_PATH}/bin/python" -m accelerate.commands.launch train_network.py \
--pretrained_model_name_or_path="${GREAT_ARMORY_PATH}/AnyLoRA" \
--dataset_config="${DATASET_CONFIG_FILE}" \
--output_dir="${OUTPUT_DIR}" \
--logging_dir="${OUTPUT_DIR}/_logs" \
--output_name="${OUTPUT_NAME}" \
--log_prefix="${OUTPUT_NAME}" \
--save_precision="fp16" \
--mixed_precision="fp16" \
--network_module=networks.lora \
--network_dim=512 \
--network_alpha=256 \
--resolution=1024 \
--train_batch_size=1 \
--max_train_epochs=15 \
--learning_rate=2e-4 \
--unet_lr=2e-4 \
--text_encoder_lr=6e-5 \
--lr_scheduler="cosine_with_restarts" \
--lr_scheduler_num_cycles=4 \
--lr_warmup_steps=0 \
--optimizer_type="AdamW8bit" \
--save_every_n_steps=10000 \
--clip_skip=2 \
--seed=42 \
--sdpa \
--cache_latents \
--no_half_vae \
--caption_extension=".txt"

echo "The forging is complete."